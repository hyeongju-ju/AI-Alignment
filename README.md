# AI-Alignment
Aligning AI systems with human intent

- [x] Papers & Web sources
* [17.03] [OpenAI]
    * Christiano et al., "Deep reinforcement learning from human preferences", NeurIPS 2017
* [17.07] [OpenAI]
    * Schulman et al., "Proximal Policy Optimization Algorithms", arXiv preprint 2017
* [18.05] [OpenAI]
    * Irving et al., "AI safety via debate", arXiv preprint 2018
* [18.10] [OpenAI]
    * Christiano et al., "Supervising strong learners by amplifying weak experts", arXiv preprint 2018
* [20.04] [OpenAI / Google Brain / MILA, etc.]
    * Brundage et al., "Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims", arXiv preprint 2020
* [20.09] [OpenAI]
    * Stiennon et al., "Learning to summarize from human feedback", NeurIPS 2020
* [21.06] [OpenAI]
    * Solaiman et al., "Process for Adapting Language Models to Society (PALMS) with Values-Targeted Datasets", NeurIPS 2021
* [21.09] [OpenAI]
    * Wu et al., "Recursively Summarizing Books with Human Feedback", arXiv preprint 2021
* [21.12] [Anthropic]
    * Askell et al., "A General Language Assistant as a Laboratory for Alignment", arXiv preprint 2021
* [21.12] [OpenAI] ["WebGPT"]
    * Nakano et al., "WebGPT: Browser-assisted question-answering with human feedback", arXiv preprint 2021
* [22.03] [OpenAI] ["InstructGPT"]
    * Ouyang et al., "Training language models to follow instructions with human feedback", arXiv preprint 2022
* [22.03] [DeepMind]
    * Menick et al., "Teaching language models to support answers with verified quotes", arXiv preprint 2022
* [22.04] [Anthropic]
    * Bai et al., "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback", arXiv preprint 2022
* [22.06] [OpenAI]
    * Saunders et al., "Self-critiquing models for assisting human evaluators", arXiv preprint 2022
* [22.07] [OpenAI]
    * Bavarian et al., "Efficient Training of Language Models to Fill in the Middle", arXiv preprint 2022
* [22.09] [DeepMind] ["Sparrow"]
    * Glaese et al., "Improving alignment of dialogue agents via targeted human judgements", arXiv preprint 2022
    * Building safer dialogue agents (Accessed on Feburary 19, 2023). 2022. URL: https://www.deepmind.com/blog/building-safer-dialogue-agents
* [22.10] [OpenAI]
    * Gao et al., "Scaling Laws for Reward Model Overoptimization", arXiv preprint 2022 (NeurIPS 2023 ?)
* [22.11] [OpenAI] ["ChatGPT"]
    * ChatGPT: Optimizing Language Models for Dialogue (Accessed on Feburary 19, 2023). 2022. URL: https://openai.com/blog/chatgpt/
    * ChatGPT (Accessed on Feburary 19, 2023). 2022. URL: https://chat.openai.com/chat
* [22-12] [Anthropic] ["Claude" ?]
    * Bai et al., "Constitutional AI: Harmlessness from AI Feedback", arXiv preprint 2022
* [23.01] [OpenAI]
    * Hilton et al., "Scaling laws for single-agent reinforcement learning", arXiv preprint 2023
* [23.02] [Google] ["Bard"]
    * An important next step on our AI journey (Accessed on Feburary 19, 2023). 2023. URL: https://blog.google/technology/ai/bard-google-ai-search-updates/
